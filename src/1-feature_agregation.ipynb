{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from scipy import stats\n",
    "\n",
    "# --------------------------\n",
    "# Configurações do dataset\n",
    "# --------------------------\n",
    "DIR_MELANOMA = r'data\\\\melanoma'\n",
    "DIR_NAEVUS   = r'data\\\\naevus'\n",
    "EXT          = ('.jpg', '.png', '.jpeg')  # ajuste se necessário\n",
    "\n",
    "# --------------------------\n",
    "# Configurações do modelo\n",
    "# --------------------------\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "BATCH_SIZE = 8  # pode ajustar conforme sua GPU/CPU\n",
    "SEED       = 42\n",
    "NFOLDS     = 5  # K-Fold estratificado\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --------------------------\n",
    "# Carregar lista de imagens + rótulos\n",
    "# --------------------------\n",
    "def list_images_from_dir(d, exts):\n",
    "    files = sorted([os.path.join(d, f) for f in os.listdir(d) if f.lower().endswith(exts)])\n",
    "    return files\n",
    "\n",
    "paths_melanoma = list_images_from_dir(DIR_MELANOMA, EXT)\n",
    "paths_naevus   = list_images_from_dir(DIR_NAEVUS, EXT)\n",
    "\n",
    "X_paths = np.array(paths_melanoma + paths_naevus)\n",
    "y       = np.array([1]*len(paths_melanoma) + [0]*len(paths_naevus))\n",
    "\n",
    "print(f\"# Imagens: {len(X_paths)} (melanoma={len(paths_melanoma)}, naevus={len(paths_naevus)})\")\n",
    "\n",
    "# --------------------------\n",
    "# Carregar ViT e processor\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = ViTModel.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# --------------------------\n",
    "# Extrair last_hidden_state para todas as imagens\n",
    "# Saída: numpy array shape (N, tokens, hidden_dim)\n",
    "# --------------------------\n",
    "def load_image_rgb(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def extract_last_hidden_states(paths, batch_size=8):\n",
    "    all_feats = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(paths), batch_size):\n",
    "            batch_paths = paths[i:i+batch_size]\n",
    "            imgs = [load_image_rgb(p) for p in batch_paths]\n",
    "            inputs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            # last_hidden_state: (B, tokens, hidden_dim)\n",
    "            feats = outputs.last_hidden_state.detach().cpu().numpy()\n",
    "            all_feats.append(feats)\n",
    "    return np.concatenate(all_feats, axis=0)\n",
    "\n",
    "print(\"Extraindo last_hidden_state...\")\n",
    "LH = extract_last_hidden_states(X_paths, batch_size=BATCH_SIZE)\n",
    "# LH shape: (N, tokens, hidden_dim) -> tokens=197, hidden_dim=768 para ViT-Base\n",
    "N, T, D = LH.shape\n",
    "print(f\"last_hidden_state shape = {LH.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# Métodos de agregação\n",
    "# Cada função retorna vetor (D,) por imagem\n",
    "# Por padrão, excetuando 'cls', agregações usam APENAS patches (tokens[1:])\n",
    "# --------------------------\n",
    "def agg_cls(x):  # x: (tokens, D)\n",
    "    return x[0]  # [CLS] token (posição 0)\n",
    "\n",
    "def agg_mean(x):\n",
    "    return x[1:].mean(axis=0)\n",
    "\n",
    "def agg_median(x):\n",
    "    return np.median(x[1:], axis=0)\n",
    "\n",
    "def agg_max(x):\n",
    "    return x[1:].max(axis=0)\n",
    "\n",
    "def agg_min(x):\n",
    "    return x[1:].min(axis=0)\n",
    "\n",
    "def agg_l2_norm(x):\n",
    "    # Norma L2 por dimensão (agregando sobre tokens): sqrt(sum_j x_j^2)\n",
    "    return np.sqrt(np.sum(np.square(x[1:]), axis=0))\n",
    "\n",
    "def agg_energy(x):\n",
    "    # Energia por dimensão (sum_j x_j^2)\n",
    "    return np.sum(np.square(x[1:]), axis=0)\n",
    "\n",
    "def agg_gem(x, p=3.0):\n",
    "    # GeM pooling por dimensão: (mean(x^p))^(1/p), tokens sem CLS\n",
    "    xp = np.power(np.clip(x[1:], a_min=0.0, a_max=None), p)  # clip para evitar negativos com p fracionário\n",
    "    return np.power(xp.mean(axis=0), 1.0/p)\n",
    "\n",
    "def agg_logsumexp(x):\n",
    "    # LogSumExp por dimensão: log(sum(exp(x)))\n",
    "    # Estável numericamente\n",
    "    a = x[1:]\n",
    "    m = np.max(a, axis=0, keepdims=True)\n",
    "    return (m + np.log(np.sum(np.exp(a - m), axis=0, keepdims=True))).ravel()\n",
    "\n",
    "AGG_FUNCS = {\n",
    "    \"cls\": agg_cls,\n",
    "    \"mean\": agg_mean,\n",
    "    \"median\": agg_median,\n",
    "    \"max\": agg_max,\n",
    "    \"min\": agg_min,\n",
    "    \"l2_norm\": agg_l2_norm,\n",
    "    \"energy\": agg_energy,\n",
    "    \"gem_p3\": lambda x: agg_gem(x, p=3.0),\n",
    "    \"logsumexp\": agg_logsumexp,\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Pré-computar features agregadas (N, D) para cada método\n",
    "# --------------------------\n",
    "def apply_aggregation_all(LH, agg_func):\n",
    "    return np.stack([agg_func(LH[i]) for i in range(LH.shape[0])], axis=0)\n",
    "\n",
    "print(\"Agregando features por método...\")\n",
    "agg_features = {name: apply_aggregation_all(LH, fn) for name, fn in AGG_FUNCS.items()}\n",
    "for k, v in agg_features.items():\n",
    "    print(f\"  {k}: {v.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# K-Fold estratificado + LDA\n",
    "# Métrica: AUC ROC (positiva=classe 1) + Accuracy\n",
    "# --------------------------\n",
    "def evaluate_lda_kfold(X, y, n_splits=5, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    aucs, accs = [], []\n",
    "    for tr_idx, te_idx in skf.split(X, y):\n",
    "        X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "        y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "        # StandardScaler por fold (fit só no treino)\n",
    "        scaler = StandardScaler()\n",
    "        X_tr = scaler.fit_transform(X_tr)\n",
    "        X_te = scaler.transform(X_te)\n",
    "\n",
    "        lda = LinearDiscriminantAnalysis(solver='svd', shrinkage=None)\n",
    "        lda.fit(X_tr, y_tr)\n",
    "        y_proba = lda.predict_proba(X_te)[:, -1]\n",
    "        y_pred  = lda.predict(X_te)\n",
    "\n",
    "        aucs.append(roc_auc_score(y_te, y_proba))\n",
    "        accs.append(accuracy_score(y_te, y_pred))\n",
    "    return np.array(aucs), np.array(accs)\n",
    "\n",
    "print(\"Avaliando LDA em K-Fold (estratificado)...\")\n",
    "results = {}\n",
    "for name, X in agg_features.items():\n",
    "    aucs, accs = evaluate_lda_kfold(X, y, n_splits=NFOLDS, seed=SEED)\n",
    "    results[name] = {\"auc\": aucs, \"acc\": accs}\n",
    "    print(f\"  {name}: AUC média={aucs.mean():.4f} ± {aucs.std():.4f} | ACC média={accs.mean():.4f} ± {accs.std():.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Teste de Friedman (usando AUC por fold)\n",
    "# --------------------------\n",
    "# Organiza AUCs por método -> forma: (k métodos, N folds)\n",
    "methods = list(results.keys())\n",
    "k = len(methods)\n",
    "auc_matrix = np.stack([results[m][\"auc\"] for m in methods], axis=0)  # (k, N)\n",
    "N = auc_matrix.shape[1]\n",
    "\n",
    "# Friedman (scipy.stats faz ranking internamente)\n",
    "friedman_stat, friedman_p = stats.friedmanchisquare(*[auc_matrix[i] for i in range(k)])\n",
    "print(\"\\n# Teste de Friedman sobre AUCs (folds como blocos)\")\n",
    "print(f\"  estatística={friedman_stat:.4f}, p-valor={friedman_p:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Nemenyi (pós-hoc) com ranks médios\n",
    "# CD = q_alpha * sqrt(k*(k+1)/(6*N)), q_alpha da distribuição studentized range\n",
    "# Tenta usar scipy.stats.studentized_range; se não houver, usa tabela aproximada\n",
    "# --------------------------\n",
    "def mean_ranks(matrix):\n",
    "    # matrix: (k, N) valores; rank por coluna (fold): melhor AUC -> rank 1\n",
    "    ranks = np.zeros_like(matrix)\n",
    "    for j in range(N):\n",
    "        col = matrix[:, j]\n",
    "        # rankdata coloca menor como 1; queremos maior (melhor) como 1 -> rank sobre -col\n",
    "        r = stats.rankdata(-col, method='average')\n",
    "        ranks[:, j] = r\n",
    "    return ranks.mean(axis=1)  # (k,)\n",
    "\n",
    "def q_alpha_nemenyi(k, alpha=0.05):\n",
    "    try:\n",
    "        # studentized_range.isf(alpha, k, np.inf) retorna q_alpha\n",
    "        from scipy.stats import studentized_range\n",
    "        return studentized_range.isf(alpha, k, np.inf)\n",
    "    except Exception:\n",
    "        # Tabela aproximada (Demsar 2006; ∞ df) para alpha=0.05\n",
    "        table = {\n",
    "            2: 1.960,  # não usual\n",
    "            3: 2.343,\n",
    "            4: 2.569,\n",
    "            5: 2.728,\n",
    "            6: 2.850,\n",
    "            7: 2.948,\n",
    "            8: 3.031,\n",
    "            9: 3.102,\n",
    "            10: 3.164,\n",
    "        }\n",
    "        return table.get(k, 3.164)  # fallback conservador\n",
    "    # Nota: valores podem variar ligeiramente conforme tabela/implementação.\n",
    "\n",
    "avg_ranks = mean_ranks(auc_matrix)  # (k,)\n",
    "order = np.argsort(avg_ranks)       # menor rank (melhor) primeiro\n",
    "sorted_methods = [methods[i] for i in order]\n",
    "sorted_ranks   = avg_ranks[order]\n",
    "\n",
    "q_alpha = q_alpha_nemenyi(k, alpha=0.05)\n",
    "CD = q_alpha * np.sqrt(k*(k+1)/(6.0*N))\n",
    "\n",
    "print(\"\\n# Nemenyi pós-hoc (baseado em ranks médios de AUC)\")\n",
    "print(f\"  k={k}, N(folds)={N}, q_alpha≈{q_alpha:.4f}, CD={CD:.4f}\")\n",
    "print(\"  Métodos ordenados por rank médio (menor=melhor):\")\n",
    "for m, r in zip(sorted_methods, sorted_ranks):\n",
    "    print(f\"    {m}: rank médio={r:.3f}\")\n",
    "\n",
    "# Pares significativamente diferentes se |rank_i - rank_j| > CD\n",
    "print(\"\\n  Pares com diferença de rank > CD (significativos a ~5%):\")\n",
    "sig_pairs = []\n",
    "for i in range(k):\n",
    "    for j in range(i+1, k):\n",
    "        diff = abs(avg_ranks[i] - avg_ranks[j])\n",
    "        if diff > CD:\n",
    "            sig_pairs.append((methods[i], methods[j], diff))\n",
    "            print(f\"    {methods[i]} vs {methods[j]}: Δrank={diff:.3f} > CD\")\n",
    "\n",
    "if not sig_pairs:\n",
    "    print(\"    Nenhum par significativo a ~5% pelo Nemenyi.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Observações:\n",
    "# - As agregações (exceto 'cls') consideram somente patches (tokens[1:]).\n",
    "# - LDA é treinado por fold com StandardScaler aplicado (fit no treino, transform no teste).\n",
    "# - Métrica principal usada para estatística é AUC; você pode trocar para ACC se preferir.\n",
    "# - Caso tenha muitos dados, considere salvar/recuperar LH pré-computado para evitar reextração.\n",
    "# --------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
